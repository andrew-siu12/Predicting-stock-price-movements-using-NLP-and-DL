{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Modelling\" data-toc-modified-id=\"Modelling-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Modelling</a></span></li><li><span><a href=\"#Basic-MLP\" data-toc-modified-id=\"Basic-MLP-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Basic MLP</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:29:33.435470Z",
     "start_time": "2019-09-10T19:29:29.028801Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 14669831303515297739\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 6672629760\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13057572494315455711\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1070 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional, GRU\n",
    "from tensorflow.keras.layers import Flatten, Dropout, Input, concatenate, BatchNormalization\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import re\n",
    "import gc\n",
    "from ast import literal_eval\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:29:33.541854Z",
     "start_time": "2019-09-10T19:29:33.437422Z"
    }
   },
   "outputs": [],
   "source": [
    "price_df = pd.read_csv(\"../inputs/preprocessed_news.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:29:33.836606Z",
     "start_time": "2019-09-10T19:29:33.544782Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = price_df['signal']\n",
    "labels = pd.get_dummies(columns=['signal'], data=labels)\n",
    "docs = price_df[\"combined_tokens\"].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:29:34.071821Z",
     "start_time": "2019-09-10T19:29:33.840511Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'aux_shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-ddea631458ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maux_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'aux_shape' is not defined"
     ]
    }
   ],
   "source": [
    "print(aux_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:29:34.082558Z",
     "start_time": "2019-09-10T19:29:29.012Z"
    }
   },
   "outputs": [],
   "source": [
    "price_df['combined_len'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:29:34.084511Z",
     "start_time": "2019-09-10T19:29:29.014Z"
    }
   },
   "outputs": [],
   "source": [
    "max_words = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:29:34.085486Z",
     "start_time": "2019-09-10T19:29:29.017Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_embedding(filename):\n",
    "    # load embedding into memory, skip first line\n",
    "    print(\"Loading Glove pre-trained model\")\n",
    "    # create a map of words to vectors\n",
    "    embedding = dict()\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as file:\n",
    "        for line in file:\n",
    "            parts = line.split()\n",
    "            word = parts[0]\n",
    "            vector = np.array(parts[1:], dtype=\"float32\")\n",
    "            # key is string word, value is numpy array for vector\n",
    "            embedding[word] = vector\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "\n",
    "def tokenize_and_pad(docs, max_words=max_words):\n",
    "    \n",
    "    t = Tokenizer()\n",
    "    t.fit_on_texts(docs)\n",
    "    docs = pad_sequences(sequences=t.texts_to_sequences(docs), maxlen=max_words, padding='post')\n",
    "    \n",
    "    vocab = t.word_index\n",
    "    \n",
    "    return docs, vocab\n",
    "\n",
    "\n",
    "# create a weight matrix for the Embedding layer from a loaded embedding\n",
    "def get_weight_matrix(embedding, vocab, embed_dim=100):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    words_not_found = []\n",
    "    \n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, embed_dim))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        embedding_vector = embedding.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            weight_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            words_not_found.append(word)\n",
    "            \n",
    "    return weight_matrix, words_not_found\n",
    "\n",
    "\n",
    "# # get vectors in the right order\n",
    "# # embedding_vectors = get_weight_matrix(raw_embedding, t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:29:34.086462Z",
     "start_time": "2019-09-10T19:29:29.019Z"
    }
   },
   "outputs": [],
   "source": [
    "docs, vocab = tokenize_and_pad(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:29:34.087439Z",
     "start_time": "2019-09-10T19:29:29.020Z"
    }
   },
   "outputs": [],
   "source": [
    "embedding_index = load_embedding(\"../inputs/glove.6B.100d.txt\")\n",
    "embedding_matrix, words_not_found = get_weight_matrix(embedding_index, vocab)\n",
    "print(f\"number of null embeddings {np.sum(np.sum(embedding_matrix, axis=1) == 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:29:34.089391Z",
     "start_time": "2019-09-10T19:29:29.022Z"
    }
   },
   "outputs": [],
   "source": [
    "del embedding_index\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:29:34.091341Z",
     "start_time": "2019-09-10T19:29:29.024Z"
    }
   },
   "outputs": [],
   "source": [
    "docs_train, docs_test, label_train, label_test = train_test_split(\n",
    "    docs, labels,\n",
    "    stratify=labels,\n",
    "    test_size=0.1,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:29:34.092318Z",
     "start_time": "2019-09-10T19:29:29.026Z"
    }
   },
   "outputs": [],
   "source": [
    "## define roc auc as our evaluation \n",
    "\n",
    "# https://stackoverflow.com/questions/41032551/how-to-compute-receiving-operating-characteristic-roc-and-auc-in-keras\n",
    "def roc_auc(y_true, y_pred):\n",
    "    value, update_op = tf.contrib.metrics.streaming_auc(y_pred, y_true)\n",
    "\n",
    "    # find all variables created for this metric\n",
    "    metric_vars = [i for i in tf.local_variables() if 'auc_roc' in i.name.split('/')[1]]\n",
    "\n",
    "    # Add metric variables to GLOBAL_VARIABLES collection.\n",
    "    # They will be initialized for new session.\n",
    "    for v in metric_vars:\n",
    "        tf.add_to_collection(tf.GraphKeys.GLOBAL_VARIABLES, v)\n",
    "\n",
    "    # force to update metric values\n",
    "    with tf.control_dependencies([update_op]):\n",
    "        value = tf.identity(value)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-10T19:29:34.094271Z",
     "start_time": "2019-09-10T19:29:29.027Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
