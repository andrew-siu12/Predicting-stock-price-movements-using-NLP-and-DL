{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "we_file = './input/w2v.npz'\n",
    "w2i_file = './input/word2idx.json'\n",
    "we_file1 = 'we.npz'\n",
    "w2i_file1 = 'w2i.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n",
      "(2001, 100)\n"
     ]
    }
   ],
   "source": [
    "npz = np.load(we_file)\n",
    "W1 = npz['arr_0']\n",
    "W2 = npz['arr_1']\n",
    "with open(w2i_file) as f:\n",
    "    word2idx = json.load(f)\n",
    "\n",
    "V = len(word2idx)\n",
    "We = (W1 + W2.T) / 2\n",
    "print(V)\n",
    "print(We.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n",
      "(2001, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 1.78013176e-01,  6.31405234e-01,  7.89888129e-02, ...,\n",
       "         1.06165849e-01, -3.56564760e-01,  9.20894742e-01],\n",
       "       [ 4.11326826e-01,  1.85065418e-01,  2.53194779e-01, ...,\n",
       "        -2.20068797e-01, -2.32921317e-01, -7.92850554e-02],\n",
       "       [ 1.16537541e-01,  3.57755542e-01,  8.43534023e-02, ...,\n",
       "         4.32217419e-01, -6.87336773e-02,  2.30459362e-01],\n",
       "       ...,\n",
       "       [-7.12446868e-02,  2.17308011e-02,  7.32104927e-02, ...,\n",
       "         8.58171508e-02,  6.98032603e-02,  1.78623311e-02],\n",
       "       [ 4.27490100e-04, -9.53731537e-02,  1.23538915e-02, ...,\n",
       "        -2.41374131e-03,  7.39913015e-03, -5.56935444e-02],\n",
       "       [ 8.93887103e-01,  3.22808623e-01, -7.57370815e-02, ...,\n",
       "         5.82668185e-01, -5.03297508e-01,  1.49342999e-01]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npz = np.load(we_file1)\n",
    "W1 = npz['arr_0']\n",
    "W2 = npz['arr_1']\n",
    "with open(w2i_file1) as f:\n",
    "    word2idx = json.load(f)\n",
    "\n",
    "V = len(word2idx)\n",
    "We = (W1 + W2.T) / 2\n",
    "print(V)\n",
    "print(We.shape)\n",
    "We"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "We_dict = {}\n",
    "for i in range(len(We)):\n",
    "    We_dict[i] = We[i]\n",
    "We_mat = We\n",
    "We = We_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "We_dict = {}\n",
    "for i in range(len(We)):\n",
    "    We_dict[i] = We[i]\n",
    "We_mat = We\n",
    "We = We_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedding matrix:  (2001, 100)\n",
      "[-0.47307502 -0.099385   -0.09589041 -0.17640852 -0.36549594 -0.3703494\n",
      " -0.02077318  0.06473521 -0.70407322  0.36368642 -0.12140581  0.10607804\n",
      " -0.38147422 -0.33595047  0.02630614  0.40290729 -0.31359473  0.22999784\n",
      " -0.09340566 -0.56674632  0.37507392 -0.15668098 -0.19009967 -0.16004563\n",
      "  0.04379853 -0.18533736 -0.41624912  0.06093006  0.21953798 -0.00269384\n",
      "  0.16223032 -0.1661843  -0.67292806  0.14945709 -0.23406345 -0.28685935\n",
      "  0.19712996  0.12334402  0.12522708 -0.44104402 -0.08649954 -0.19113574\n",
      "  0.02770183 -0.03364918 -0.42664094 -0.16849571  0.14111816  0.56005694\n",
      "  0.63843604 -0.4113533  -0.21880957 -0.49783617 -0.46450638 -0.25481645\n",
      "  0.82416285 -0.35342504  0.04085545 -0.33044721  0.49836665 -0.30800719\n",
      " -0.29696197  0.51098821  0.15383551 -0.70191062 -0.27299146 -0.29249507\n",
      " -0.40844042  0.01977578 -0.08386517 -0.08116732 -0.5981525   0.65475875\n",
      " -0.28976753 -0.41788973  0.77847077  0.58362322 -0.08157273  0.08672003\n",
      " -0.28715051  0.06877849 -0.42927881  0.66753724  0.0565544   0.55282175\n",
      "  0.37598023  0.03245512 -0.43960593  0.29932653  0.23867823  0.15888465\n",
      " -0.43668652  0.48647394 -0.03430317  0.18902525  0.13808322  0.20625555\n",
      " -0.17722934  0.43926817 -0.53621149 -0.45263986]\n"
     ]
    }
   ],
   "source": [
    "print(\"Word embedding matrix: \",We_mat.shape)\n",
    "#print(\"Word embedding lookup: \", We[0])\n",
    "#print(We[word2idx['google']])\n",
    "#print(We[word2idx['apple']])\n",
    "print(We[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embedding matrix:  (2001, 100)\n",
      "[ 0.17801318  0.63140523  0.07898881 -0.08665894 -0.2505459  -0.39944652\n",
      "  0.20587009  0.2815586  -0.2013742   0.16353582 -0.3351771   0.09204864\n",
      "  0.10391462 -0.7370548  -0.06874743 -0.25299126  0.04814132 -0.4864385\n",
      "  0.05191832  0.24239847  0.24985951 -0.06326461 -0.07911653  0.0884959\n",
      "  0.36331448 -0.10132605 -0.36988443  0.16352491  0.2164477   0.15869856\n",
      "  0.30769095  0.21356174  0.30921417  0.26346585 -0.06366834 -0.50179076\n",
      " -0.15469812 -0.21681294 -0.42695728 -0.09561577 -0.22243308  0.57176876\n",
      "  0.23268417 -0.28996158 -0.29300043 -0.03369722  0.00805856  0.20851883\n",
      "  0.17256545 -0.21423195 -0.23505089  0.09192784  0.10179603  0.1793714\n",
      "  0.21436909  0.13721791  0.38384688 -0.08623943 -0.28696716 -0.06335443\n",
      " -0.11013348 -0.40471065  0.06697717  0.0286451  -0.53794736  0.4272172\n",
      " -0.1059827   0.7993795  -0.1932934   0.35362935 -0.2087036   0.22012429\n",
      "  0.19687384  0.15277886 -0.51006484 -0.11172552  0.17923819  0.03326314\n",
      " -0.3560758  -0.3917179  -0.45471513 -0.54865193 -0.11710352  0.6005454\n",
      " -0.06342211  0.5698155  -0.05602052 -0.02616471 -0.02661141  0.18879154\n",
      " -0.14117458  0.0252762  -0.28408253  0.0417697   0.18997979  0.10496504\n",
      " -0.11200975  0.10616585 -0.35656476  0.92089474]\n"
     ]
    }
   ],
   "source": [
    "print(\"Word embedding matrix: \",We_mat.shape)\n",
    "#print(\"Word embedding lookup: \", We[0])\n",
    "#print(We[word2idx['google']])\n",
    "#print(We[word2idx['apple']])\n",
    "print(We[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n"
     ]
    }
   ],
   "source": [
    "word_index = max(word2idx.values()) + 1\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2001\n"
     ]
    }
   ],
   "source": [
    "word_index = max(word2idx.values()) + 1\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('openie.p', 'rb') as infile:\n",
    "        openie = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('openie1.p', 'rb') as infile:\n",
    "        openie = pickle.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('GOOG', '20180620', 'us lawmakers want google to reconsider links to china huawei'), [{'subject': 'us lawmakers', 'subjectSpan': [0, 2], 'relation': 'want', 'relationSpan': [2, 3], 'object': 'google', 'objectSpan': [3, 4]}, {'subject': 'google', 'subjectSpan': [3, 4], 'relation': 'reconsider', 'relationSpan': [5, 6], 'object': 'links to china huawei', 'objectSpan': [6, 10]}, {'subject': 'google', 'subjectSpan': [3, 4], 'relation': 'reconsider', 'relationSpan': [5, 6], 'object': 'links', 'objectSpan': [6, 7]}]), (('GOOG', '20180621', 'youtube pushes memberships merchandise as alternatives to ads'), [{'subject': 'youtube', 'subjectSpan': [0, 1], 'relation': 'pushes', 'relationSpan': [1, 2], 'object': 'memberships merchandise', 'objectSpan': [2, 4]}, {'subject': 'youtube', 'subjectSpan': [0, 1], 'relation': 'pushes memberships merchandise as', 'relationSpan': [1, 5], 'object': 'alternatives to ads', 'objectSpan': [5, 8]}, {'subject': 'youtube', 'subjectSpan': [0, 1], 'relation': 'pushes memberships merchandise as', 'relationSpan': [1, 5], 'object': 'alternatives', 'objectSpan': [5, 6]}])]\n"
     ]
    }
   ],
   "source": [
    "#(title, openie info)\n",
    "print(list(openie.items())[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('GOOGL', '20171102', 'us lawmakers release sample of russianbought facebook ads'), [{'subject': 'us', 'subjectSpan': [0, 1], 'relation': 'release', 'relationSpan': [2, 3], 'object': 'sample', 'objectSpan': [3, 4]}, {'subject': 'us', 'subjectSpan': [0, 1], 'relation': 'release', 'relationSpan': [2, 3], 'object': 'sample of russianbought facebook ads', 'objectSpan': [3, 8]}, {'subject': 'lawmakers', 'subjectSpan': [1, 2], 'relation': 'release', 'relationSpan': [2, 3], 'object': 'sample of russianbought facebook ads', 'objectSpan': [3, 8]}, {'subject': 'lawmakers', 'subjectSpan': [1, 2], 'relation': 'release', 'relationSpan': [2, 3], 'object': 'sample', 'objectSpan': [3, 4]}]), (('GOOGL', '20171031', 'google ditched autopilot driving feature after test user napped behind wheel'), [{'subject': 'google', 'subjectSpan': [0, 1], 'relation': 'ditched', 'relationSpan': [1, 2], 'object': 'autopilot', 'objectSpan': [2, 3]}, {'subject': 'test user', 'subjectSpan': [6, 8], 'relation': 'napped behind', 'relationSpan': [8, 10], 'object': 'wheel', 'objectSpan': [10, 11]}, {'subject': 'autopilot', 'subjectSpan': [2, 3], 'relation': 'driving', 'relationSpan': [3, 4], 'object': 'feature', 'objectSpan': [4, 5]}, {'subject': 'autopilot', 'subjectSpan': [2, 3], 'relation': 'driving feature', 'relationSpan': [3, 5], 'object': 'test user napped', 'objectSpan': [6, 9]}, {'subject': 'autopilot', 'subjectSpan': [2, 3], 'relation': 'driving feature', 'relationSpan': [3, 5], 'object': 'test user napped behind wheel', 'objectSpan': [6, 11]}])]\n"
     ]
    }
   ],
   "source": [
    "#(title, openie info)\n",
    "print(list(openie.items())[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "info2index = {}\n",
    "index2info = []\n",
    "word_emb = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "info2index = {}\n",
    "index2info = []\n",
    "word_emb = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in openie:\n",
    "    #print(k)\n",
    "    info2index[k] = index\n",
    "    index += 1\n",
    "    index2info += [k]\n",
    "    \n",
    "    # event tuples up to 10 event tuples\n",
    "    emb = np.zeros((3, 100))\n",
    "    for e in openie[k]:\n",
    "        #print(e)\n",
    "        subject = e['subject'].split(' ')\n",
    "        for w in subject:\n",
    "            if w not in word2idx:\n",
    "                word2idx[w] = word_index\n",
    "                word_index += 1\n",
    "            if w not in We:\n",
    "                We[word2idx[w]] = np.random.randn(100) * .1\n",
    "            emb[0] = We[word2idx[w]]\n",
    "        relation = e['relation'].split(' ')\n",
    "        for w in relation:\n",
    "            if w not in word2idx:\n",
    "                word2idx[w] = word_index\n",
    "                word_index += 1\n",
    "            if word2idx[w] not in We:\n",
    "                We[word2idx[w]] = np.random.randn(100) * .1\n",
    "            emb[1] = We[word2idx[w]]\n",
    "        obj = e['object'].split(' ')\n",
    "        for w in obj:\n",
    "            if w not in word2idx:\n",
    "                word2idx[w] = word_index\n",
    "                word_index += 1\n",
    "            if word2idx[w] not in We:\n",
    "                We[word2idx[w]] = np.random.randn(100) * .1\n",
    "            emb[2] = We[word2idx[w]]\n",
    "        emb /= len(openie[k])\n",
    "        word_emb += [emb]\n",
    "with open('run_info.p', 'wb') as outfile:\n",
    "    pickle.dump({\n",
    "        'We_mat': We_mat,\n",
    "        'We_dict': We_dict,\n",
    "        'word2idx': word2idx,\n",
    "        'word_emb': word_emb,\n",
    "        'info2index': info2index,\n",
    "        'index2info': index2info\n",
    "    }, outfile)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in openie:\n",
    "    #print(k)\n",
    "    info2index[k] = index\n",
    "    index += 1\n",
    "    index2info += [k]\n",
    "    \n",
    "    # event tuples up to 10 event tuples\n",
    "    emb = np.zeros((3, 100))\n",
    "    for e in openie[k]:\n",
    "        #print(e)\n",
    "        subject = e['subject'].split(' ')\n",
    "        for w in subject:\n",
    "            if w not in word2idx:\n",
    "                word2idx[w] = word_index\n",
    "                word_index += 1\n",
    "            if w not in We:\n",
    "                We[word2idx[w]] = np.random.randn(100) * .1\n",
    "            emb[0] = We[word2idx[w]]\n",
    "        relation = e['relation'].split(' ')\n",
    "        for w in relation:\n",
    "            if w not in word2idx:\n",
    "                word2idx[w] = word_index\n",
    "                word_index += 1\n",
    "            if word2idx[w] not in We:\n",
    "                We[word2idx[w]] = np.random.randn(100) * .1\n",
    "            emb[1] = We[word2idx[w]]\n",
    "        obj = e['object'].split(' ')\n",
    "        for w in obj:\n",
    "            if w not in word2idx:\n",
    "                word2idx[w] = word_index\n",
    "                word_index += 1\n",
    "            if word2idx[w] not in We:\n",
    "                We[word2idx[w]] = np.random.randn(100) * .1\n",
    "            emb[2] = We[word2idx[w]]\n",
    "        emb /= len(openie[k])\n",
    "        word_emb += [emb]\n",
    "with open('run_info1.p', 'wb') as outfile:\n",
    "    pickle.dump({\n",
    "        'We_mat': We_mat,\n",
    "        'We_dict': We_dict,\n",
    "        'word2idx': word2idx,\n",
    "        'word_emb': word_emb,\n",
    "        'info2index': info2index,\n",
    "        'index2info': index2info\n",
    "    }, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21690, 3, 100)\n",
      "7034 7649\n"
     ]
    }
   ],
   "source": [
    "word_emb = np.array(word_emb)\n",
    "print(word_emb.shape)\n",
    "print(index, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18698, 3, 100)\n",
      "6122 6935\n"
     ]
    }
   ],
   "source": [
    "word_emb = np.array(word_emb)\n",
    "print(word_emb.shape)\n",
    "print(index, word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18698, 3, 100)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(word_emb*5).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.08698172,  0.00748662, -0.03750919, -0.03022153, -0.00073084,\n",
       "         0.01935744,  0.03024965,  0.04260419, -0.02512739, -0.02290657,\n",
       "         0.00360637, -0.00750835,  0.03180132, -0.04990246,  0.04887452,\n",
       "         0.01757911,  0.03434047,  0.0122204 , -0.04516889,  0.01648081,\n",
       "         0.05984187,  0.0177042 ,  0.00453863,  0.03933694,  0.02178326,\n",
       "        -0.00160026,  0.02246258, -0.01665264,  0.02317476, -0.04444312,\n",
       "         0.02056673,  0.00696279, -0.00215316, -0.05703395,  0.02731256,\n",
       "        -0.00385006, -0.02287973,  0.01559905, -0.02000393, -0.00669876,\n",
       "        -0.04330966,  0.01474018,  0.0127074 , -0.01337701,  0.02745002,\n",
       "         0.07228637, -0.02350692, -0.02886113,  0.04671321,  0.03891409,\n",
       "         0.04562575, -0.04372178, -0.02945624,  0.05331724, -0.03420386,\n",
       "         0.03104771, -0.02148562,  0.06126799,  0.02867829, -0.04052577,\n",
       "        -0.01043321,  0.02425571, -0.05009015,  0.02031696, -0.00779936,\n",
       "        -0.05440257, -0.02457113, -0.02562844,  0.04443831, -0.02189536,\n",
       "         0.02673063,  0.03577246, -0.03084233, -0.0361575 , -0.01489767,\n",
       "        -0.0122079 , -0.06114937,  0.00315566,  0.0462544 ,  0.0247836 ,\n",
       "         0.05434983,  0.03360652,  0.06145327,  0.03128835,  0.03054826,\n",
       "         0.0100729 ,  0.02169898,  0.03112514, -0.02732535, -0.04513544,\n",
       "         0.00288782,  0.02222598,  0.04368573,  0.0351896 , -0.03634521,\n",
       "         0.0065345 ,  0.03952802,  0.08217972,  0.07744818,  0.00777005],\n",
       "       [ 0.02204942,  0.00389636,  0.00750575, -0.0068618 ,  0.01337828,\n",
       "         0.01991619, -0.00343388, -0.01186898, -0.00726954, -0.00175812,\n",
       "         0.00613374, -0.00456566,  0.00087178,  0.00483769,  0.00512399,\n",
       "        -0.0011076 ,  0.01526598,  0.01519959, -0.0083238 ,  0.01627553,\n",
       "         0.00423464,  0.0034589 , -0.02111416,  0.0122998 , -0.02684238,\n",
       "         0.001201  ,  0.00596171,  0.00700946, -0.00559258, -0.00680815,\n",
       "        -0.00902055, -0.00048144,  0.01721575, -0.00386008,  0.00628518,\n",
       "         0.00680664, -0.0042971 , -0.0097258 ,  0.00121745, -0.00038651,\n",
       "        -0.00366482,  0.00241641, -0.00673411, -0.00041713,  0.01505191,\n",
       "         0.00945261, -0.01747391, -0.00240856,  0.00182756,  0.00371149,\n",
       "         0.01309855,  0.00713117,  0.00265889,  0.0095137 ,  0.00853033,\n",
       "         0.01566158, -0.00946518, -0.00440827, -0.02020882,  0.0089494 ,\n",
       "         0.00303356, -0.02103658, -0.01600505,  0.0105656 ,  0.02046048,\n",
       "         0.00215358,  0.0223535 ,  0.01272218, -0.00782219, -0.00089134,\n",
       "         0.00985618, -0.02700175,  0.00266059, -0.00276853,  0.00897769,\n",
       "        -0.00607559,  0.00512472,  0.01261585,  0.0039837 ,  0.02260825,\n",
       "         0.00244485,  0.00178473, -0.00284641, -0.01542894, -0.00104424,\n",
       "        -0.00811537,  0.02630172, -0.00613124, -0.01813916, -0.01003159,\n",
       "         0.01620502, -0.02358706, -0.01060984, -0.00762188, -0.00162393,\n",
       "        -0.00952102, -0.00230709, -0.0083956 ,  0.00105454, -0.01999542],\n",
       "       [ 0.01575679,  0.04376403,  0.01186916,  0.03078028,  0.04676693,\n",
       "         0.01399686,  0.01938083, -0.03028084, -0.00368078, -0.07792299,\n",
       "         0.03818439,  0.02941944, -0.06343487, -0.00059411,  0.01564253,\n",
       "         0.01838193, -0.0057885 ,  0.0186624 , -0.05441598,  0.01983494,\n",
       "         0.07467346, -0.02746238, -0.0587426 ,  0.00585649,  0.05658182,\n",
       "        -0.01711752,  0.0360888 ,  0.01789891, -0.00740114,  0.03111362,\n",
       "         0.04997598, -0.02247304,  0.02193325,  0.06383964, -0.03015745,\n",
       "         0.00342525,  0.03845268, -0.00447209,  0.04319536,  0.03740349,\n",
       "         0.02857726,  0.00336062,  0.01728245, -0.02484645, -0.04621347,\n",
       "        -0.01359338,  0.03024053,  0.02573825,  0.02189368,  0.07674703,\n",
       "        -0.01472725, -0.0004762 ,  0.04285512,  0.0549546 , -0.0073235 ,\n",
       "         0.01457727, -0.01477149,  0.06063916,  0.0149327 ,  0.00263787,\n",
       "         0.02523899, -0.03727898, -0.02943792,  0.04021565, -0.00803698,\n",
       "         0.00653806,  0.02024584,  0.04016823, -0.03238146,  0.01082968,\n",
       "        -0.04644143,  0.00775219, -0.05120231, -0.04314912,  0.00106223,\n",
       "         0.00155134, -0.01284293,  0.01319875,  0.04125032, -0.00474132,\n",
       "         0.00821683,  0.07329614,  0.02895942, -0.00304428,  0.04070742,\n",
       "         0.01784896,  0.01126145,  0.05861383,  0.00455903,  0.01128243,\n",
       "        -0.02922889, -0.01697422, -0.05323215, -0.0165169 ,  0.00531389,\n",
       "         0.01117198, -0.00274545, -0.0113158 ,  0.01872156, -0.01306371]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reference https://github.com/lazyprogrammer/machine_learning_examples/blob/master/nlp_class2/glove.py\n",
    "\n",
    "class Glove(object):\n",
    "    def __init__(self, D, V, context_sz):\n",
    "        self.D = D\n",
    "        self.V = V\n",
    "        self.context_sz = context_sz\n",
    "\n",
    "    def fit(self, sentences, cc_matrix=None, learning_rate=10e-5, reg=0.1, xmax=100, alpha=0.75, epochs=10, gd=False, use_theano=True):\n",
    "        # build co-occurrence matrix\n",
    "        # paper calls it X, so we will call it X, instead of calling\n",
    "        # the training data X\n",
    "        # TODO: would it be better to use a sparse matrix?\n",
    "        t0 = datetime.now()\n",
    "        V = self.V\n",
    "        D = self.D\n",
    "\n",
    "        if os.path.exists(cc_matrix):\n",
    "            X = np.load(cc_matrix)\n",
    "        else:\n",
    "            X = np.zeros((V, V))\n",
    "            N = len(sentences)\n",
    "            print(\"number of sentences to process:\", N)\n",
    "            it = 0\n",
    "            for sentence in sentences:\n",
    "                it += 1\n",
    "                if it % 10000 == 0:\n",
    "                    print(\"processed\", it, \"/\", N)\n",
    "                n = len(sentence)\n",
    "                for i in range(n):\n",
    "                    wi = sentence[i]\n",
    "\n",
    "                    start = max(0, i - self.context_sz)\n",
    "                    end = min(n, i + self.context_sz)\n",
    "\n",
    "                    # we can either choose only one side as context, or both\n",
    "                    # here we are doing both\n",
    "\n",
    "                    # make sure \"start\" and \"end\" tokens are part of some context\n",
    "                    # otherwise their f(X) will be 0 (denominator in bias update)\n",
    "                    if i - self.context_sz < 0:\n",
    "                        points = 1.0 / (i + 1)\n",
    "                        X[wi,0] += points\n",
    "                        X[0,wi] += points\n",
    "                    if i + self.context_sz > n:\n",
    "                        points = 1.0 / (n - i)\n",
    "                        X[wi,1] += points\n",
    "                        X[1,wi] += points\n",
    "\n",
    "                    for j in range(start, i):\n",
    "                        if j == i: continue\n",
    "                        wj = sentence[j]\n",
    "                        points = 1.0 / abs(i - j) # this is +ve\n",
    "                        X[wi,wj] += points\n",
    "                        X[wj,wi] += points\n",
    "            # save the cc matrix because it takes forever to create\n",
    "            np.save(cc_matrix, X)\n",
    "\n",
    "        print(\"max in X:\", X.max())\n",
    "\n",
    "        # weighting\n",
    "        fX = np.zeros((V, V))\n",
    "        fX[X < xmax] = (X[X < xmax] / float(xmax)) ** alpha\n",
    "        fX[X >= xmax] = 1\n",
    "\n",
    "        print(\"max in f(X):\", fX.max())\n",
    "\n",
    "        # target\n",
    "        logX = np.log(X + 1)\n",
    "\n",
    "        print(\"max in log(X):\", logX.max())\n",
    "\n",
    "        print(\"time to build co-occurrence matrix:\", (datetime.now() - t0))\n",
    "\n",
    "        # initialize weights\n",
    "        W = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "        b = np.zeros(V)\n",
    "        U = np.random.randn(V, D) / np.sqrt(V + D)\n",
    "        c = np.zeros(V)\n",
    "        mu = logX.mean()\n",
    "\n",
    "        if gd and use_theano:\n",
    "            thW = theano.shared(W)\n",
    "            thb = theano.shared(b)\n",
    "            thU = theano.shared(U)\n",
    "            thc = theano.shared(c)\n",
    "            thLogX = T.matrix('logX')\n",
    "            thfX = T.matrix('fX')\n",
    "\n",
    "            params = [thW, thb, thU, thc]\n",
    "\n",
    "            thDelta = thW.dot(thU.T) + T.reshape(thb, (V, 1)) + T.reshape(thc, (1, V)) + mu - thLogX\n",
    "            thCost = ( thfX * thDelta * thDelta ).sum()\n",
    "\n",
    "            grads = T.grad(thCost, params)\n",
    "\n",
    "            updates = [(p, p - learning_rate*g) for p, g in zip(params, grads)]\n",
    "\n",
    "            train_op = theano.function(\n",
    "                inputs=[thfX, thLogX],\n",
    "                updates=updates,\n",
    "            )\n",
    "\n",
    "        costs = []\n",
    "        sentence_indexes = range(len(sentences))\n",
    "        for epoch in range(epochs):\n",
    "            delta = W.dot(U.T) + b.reshape(V, 1) + c.reshape(1, V) + mu - logX\n",
    "            cost = ( fX * delta * delta ).sum()\n",
    "            costs.append(cost)\n",
    "            print(\"epoch:\", epoch, \"cost:\", cost)\n",
    "\n",
    "            if gd:\n",
    "                # gradient descent method\n",
    "\n",
    "                if use_theano:\n",
    "                    train_op(fX, logX)\n",
    "                    W = thW.get_value()\n",
    "                    b = thb.get_value()\n",
    "                    U = thU.get_value()\n",
    "                    c = thc.get_value()\n",
    "\n",
    "                else:\n",
    "                    # update W\n",
    "                    oldW = W.copy()\n",
    "                    for i in range(V):\n",
    "                        W[i] -= learning_rate*(fX[i,:]*delta[i,:]).dot(U)\n",
    "                    W -= learning_rate*reg*W\n",
    "\n",
    "                    # update b\n",
    "                    for i in range(V):\n",
    "                        b[i] -= learning_rate*fX[i,:].dot(delta[i,:])\n",
    "                    b -= learning_rate*reg*b\n",
    "\n",
    "                    # update U\n",
    "                    for j in range(V):\n",
    "                        U[j] -= learning_rate*(fX[:,j]*delta[:,j]).dot(oldW)\n",
    "                    U -= learning_rate*reg*U\n",
    "\n",
    "                    # update c\n",
    "                    for j in range(V):\n",
    "                        c[j] -= learning_rate*fX[:,j].dot(delta[:,j])\n",
    "                    c -= learning_rate*reg*c\n",
    "\n",
    "            else:\n",
    "                # ALS method\n",
    "\n",
    "                # update W\n",
    "                # fast way\n",
    "                # t0 = datetime.now()\n",
    "                for i in range(V):\n",
    "                    # matrix = reg*np.eye(D) + np.sum((fX[i,j]*np.outer(U[j], U[j]) for j in range(V)), axis=0)\n",
    "                    matrix = reg*np.eye(D) + (fX[i,:]*U.T).dot(U)\n",
    "                    # assert(np.abs(matrix - matrix2).sum() < 10e-5)\n",
    "                    vector = (fX[i,:]*(logX[i,:] - b[i] - c - mu)).dot(U)\n",
    "                    W[i] = np.linalg.solve(matrix, vector)\n",
    "                # print(\"fast way took:\", (datetime.now() - t0))\n",
    "\n",
    "                # update b\n",
    "                for i in range(V):\n",
    "                    denominator = fX[i,:].sum()\n",
    "                    # assert(denominator > 0)\n",
    "                    numerator = fX[i,:].dot(logX[i,:] - W[i].dot(U.T) - c - mu)\n",
    "                    # for j in range(V):\n",
    "                    #     numerator += fX[i,j]*(logX[i,j] - W[i].dot(U[j]) - c[j])\n",
    "                    b[i] = numerator / denominator / (1 + reg)\n",
    "                # print(\"updated b\")\n",
    "\n",
    "                # update U\n",
    "                for j in range(V):\n",
    "                    matrix = reg*np.eye(D) + (fX[:,j]*W.T).dot(W)\n",
    "                    vector = (fX[:,j]*(logX[:,j] - b - c[j] - mu)).dot(W)\n",
    "                    U[j] = np.linalg.solve(matrix, vector)\n",
    "\n",
    "                # update c\n",
    "                for j in range(V):\n",
    "                    denominator = fX[:,j].sum()\n",
    "                    numerator = fX[:,j].dot(logX[:,j] - W.dot(U[j]) - b  - mu)\n",
    "                    c[j] = numerator / denominator / (1 + reg)\n",
    "\n",
    "        self.W = W\n",
    "        self.U = U\n",
    "\n",
    "        plt.plot(costs)\n",
    "        plt.show()\n",
    "\n",
    "    def save(self, fn):\n",
    "        # function word_analogies expects a (V,D) matrx and a (D,V) matrix\n",
    "        arrays = [self.W, self.U.T]\n",
    "        np.savez(fn, *arrays)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(we_file, w2i_file, sen):\n",
    "    headlines = []\n",
    "    for t in titles:\n",
    "        headlines.append(t[2])\n",
    "    token_sent = []\n",
    "    for t in headlines:\n",
    "        tokens = word_tokenize(t)\n",
    "        token_sent.append(tokens)\n",
    "    cc_matrix = \"./input/cc_matrix1.npy\"\n",
    "    if not os.path.isfile(w2i_file):\n",
    "        sentences, word2idx = get_reuters_data(5000, token_sent)\n",
    "        with open(w2i_file, 'w') as f:\n",
    "            json.dump(word2idx, f)\n",
    "        with open(sen, 'w') as f:\n",
    "            json.dump(sentences, f)\n",
    "    else:\n",
    "        with open(w2i_file) as data_file:    \n",
    "            word2idx = json.load(data_file)\n",
    "        with open(sen) as data_file:    \n",
    "            sentences = json.load(data_file)\n",
    "    \n",
    "    V = len(word2idx)\n",
    "    model = Glove(100, V, 10)\n",
    "    # model.fit(sentences, cc_matrix=cc_matrix, epochs=20) # ALS\n",
    "    model.fit(\n",
    "        sentences,\n",
    "        cc_matrix=cc_matrix,\n",
    "        learning_rate=3*10e-5,\n",
    "        reg=0.01,\n",
    "        epochs=2000,\n",
    "        gd=True,\n",
    "        use_theano=True\n",
    "    ) # gradient descent\n",
    "    model.save(we_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reuters_data(n_vocab, token_sent):\n",
    "    # return variables\n",
    "    sentences = []\n",
    "    word2idx = {'START': 0, 'END': 1}\n",
    "    idx2word = ['START', 'END']\n",
    "    current_idx = 2\n",
    "    word_idx_count = {0: float('inf'), 1: float('inf')}\n",
    "    tag = 0\n",
    "    for sentence in token_sent:\n",
    "        tokens = [unify_word(t) for t in sentence]\n",
    "        for t in tokens:\n",
    "            if t not in word2idx:\n",
    "                word2idx[t] = current_idx\n",
    "                idx2word.append(t)\n",
    "                current_idx += 1\n",
    "            idx = word2idx[t]\n",
    "            word_idx_count[idx] = word_idx_count.get(idx, 0) + 1\n",
    "        sentence_by_idx = [word2idx[t] for t in tokens]\n",
    "        sentences.append(sentence_by_idx)\n",
    "        tag += 1\n",
    "     # restrict vocab size\n",
    "    sorted_word_idx_count = sorted(word_idx_count.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    word2idx_small = {}\n",
    "    new_idx = 0\n",
    "    idx_new_idx_map = {}\n",
    "    for idx, count in sorted_word_idx_count[:n_vocab]:\n",
    "        word = idx2word[idx]\n",
    "        #print(word, count)\n",
    "        word2idx_small[word] = new_idx\n",
    "        idx_new_idx_map[idx] = new_idx\n",
    "        new_idx += 1\n",
    "    # let 'unknown' be the last token\n",
    "    word2idx_small['UNKNOWN'] = new_idx \n",
    "    unknown = new_idx\n",
    "\n",
    "    # map old idx to new idx\n",
    "    sentences_small = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) > 1:\n",
    "            new_sentence = [idx_new_idx_map[idx] if idx in idx_new_idx_map else unknown for idx in sentence]\n",
    "            sentences_small.append(new_sentence)\n",
    "\n",
    "    return sentences_small, word2idx_small\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
